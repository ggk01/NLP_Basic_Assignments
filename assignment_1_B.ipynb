{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î•Î Î•ÎÎ•Î¡Î“Î‘Î£Î™Î‘ Î¦Î¥Î£Î™ÎšÎ—Î£ Î“Î›Î©Î£Î£Î‘Î£ - Î•ÏÎ³Î±ÏƒÎ¯Î± 1\n",
    "## Î’. N-gram Language Models\n",
    "**ÎœÎ¬Î¸Î·Î¼Î±:**  Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î“Î»ÏÏƒÏƒÎ±Ï‚ \n",
    "\n",
    "**Î£Ï…Î³Î³ÏÎ±Ï†Î­Î±Ï‚:** Î™Ï‰Î¬Î½Î½Î·Ï‚ KÎ¿Ï…Ï„ÏƒÎ¿ÏÎºÎ·Ï‚ \n",
    "\n",
    "**Î•ÎºÎ´ÏŒÏƒÎµÎ¹Ï‚:** 2025-03-22 | v.0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Î’Î®Î¼Î± 1: Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… Corpus\n",
    "\n",
    "Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î¿ corpus `treebank` Î±Ï€ÏŒ Ï„Î¿ NLTK, Ï„Î¿ Î¿Ï€Î¿Î¯Î¿ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ 199 Î±ÏÏ‡ÎµÎ¯Î± ÎµÎ¹Î´Î®ÏƒÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î·Î½ Wall Street Journal. Î”Î¹Î±Ï‡Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÎµ:\n",
    "\n",
    "- **170 Î±ÏÏ‡ÎµÎ¯Î±** Î³Î¹Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· (training set).\n",
    "- **29 Î±ÏÏ‡ÎµÎ¯Î±** Î³Î¹Î± Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· (test set).\n",
    "\n",
    "Î¤Î¿ corpus ÎµÎ¯Î½Î±Î¹ Î®Î´Î· Ï‡Ï‰ÏÎ¹ÏƒÎ¼Î­Î½Î¿ ÏƒÎµ tokens ÎºÎ±Î¹ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/ioanniskoutsoukis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î Î»Î®Î¸Î¿Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚: 3509\n",
      "Î Î»Î®Î¸Î¿Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚: 405\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter\n",
    "\n",
    "# ğŸ“Œ ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± corpus treebank Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹\n",
    "nltk.download('treebank')\n",
    "\n",
    "# ğŸ”¹ Î›Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î± 199 Î±ÏÏ‡ÎµÎ¯Î±\n",
    "files = treebank.fileids()\n",
    "\n",
    "# ğŸ”¹ Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÎµ train ÎºÎ±Î¹ test\n",
    "train_files = files[:170]\n",
    "test_files = files[170:]\n",
    "\n",
    "# ğŸ”¹ Î•Î¾Î±Î³Ï‰Î³Î® Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½\n",
    "train_sents = [sent for file in train_files for sent in treebank.sents(file)]\n",
    "test_sents = [sent for file in test_files for sent in treebank.sents(file)]\n",
    "\n",
    "print(f\"Î Î»Î®Î¸Î¿Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚: {len(train_sents)}\")\n",
    "print(f\"Î Î»Î®Î¸Î¿Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚: {len(test_sents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î’Î®Î¼Î± 2: Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î›ÎµÎ¾Î¹Î»Î¿Î³Î¯Î¿Ï… ÎºÎ±Î¹ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î¼Îµ `<UNK>`\n",
    "\n",
    "Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÏƒÏ…Ï‡Î½ÏŒÏ„Î·Ï„ÎµÏ‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·Ï‚ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ tokens Î±Ï€ÏŒ Ï„Î¿ ÏƒÏÎ½Î¿Î»Î¿ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚ ÎºÎ±Î¹ ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Î»ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿ **L**:\n",
    "\n",
    "- Î¤Î± tokens Ï€Î¿Ï… ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ **Î»Î¹Î³ÏŒÏ„ÎµÏÎµÏ‚ Î±Ï€ÏŒ 3 Ï†Î¿ÏÎ­Ï‚** Î±Î½Ï„Î¹ÎºÎ±Î¸Î¯ÏƒÏ„Î±Î½Ï„Î±Î¹ Î¼Îµ `<UNK>`.\n",
    "- Î¤Î± Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î± Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½Î¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ Î»ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿ **L**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¹ Î£Ï…Î³ÎºÎ­Î½Ï„ÏÏ‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ tokens Î±Ï€ÏŒ Ï„Î± ÎºÎµÎ¯Î¼ÎµÎ½Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚\n",
    "train_tokens = [token for sent in train_sents for token in sent]\n",
    "\n",
    "# ğŸ”¹ ÎœÎ­Ï„ÏÎ·ÏƒÎ· ÏƒÏ…Ï‡Î½Î¿Ï„Î®Ï„Ï‰Î½\n",
    "freqs = Counter(train_tokens)\n",
    "\n",
    "# ğŸ”¹ ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Î»ÎµÎ¾Î¹Î»Î¿Î³Î¯Î¿Ï…: ÎºÏÎ±Ï„Î¬Î¼Îµ ÏŒ,Ï„Î¹ Î­Ï‡ÎµÎ¹ ÏƒÏ…Ï‡Î½ÏŒÏ„Î·Ï„Î± â‰¥ 3\n",
    "vocab = {word for word, count in freqs.items() if count >= 3}\n",
    "\n",
    "# ğŸ”¹ Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚ <UNK>\n",
    "def replace_with_unk(sent, vocab):\n",
    "    return [token if token in vocab else \"<UNK>\" for token in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î’Î®Î¼Î± 3: Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ `<BOS>` ÎºÎ±Î¹ `<EOS>`\n",
    "\n",
    "ÎšÎ¬Î¸Îµ Ï€ÏÏŒÏ„Î±ÏƒÎ· Ï€ÎµÏÎ¹Î²Î¬Î»Î»ÎµÏ„Î±Î¹ Î±Ï€ÏŒ Ï„Î± ÎµÎ¹Î´Î¹ÎºÎ¬ tokens:\n",
    "\n",
    "- `<BOS>` ÏƒÏ„Î·Î½ Î±ÏÏ‡Î® Ï„Î·Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.\n",
    "- `<EOS>` ÏƒÏ„Î¿ Ï„Î­Î»Î¿Ï‚ Ï„Î·Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.\n",
    "\n",
    "Î‘Ï…Ï„ÏŒ Î¼Î±Ï‚ ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î½-Î³ÏÎ¬Î¼Î¼Î±Ï„Î± Ï€Î¿Ï… ÏƒÎ­Î²Î¿Î½Ï„Î±Î¹ Ï„Î± ÏŒÏÎ¹Î± Ï„Î·Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚. Î— Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î¼Îµ `<UNK>` Î³Î¯Î½ÎµÏ„Î±Î¹ Ï€ÏÎ¹Î½ Ï„Î·Î½ Ï€ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Ï„Ï‰Î½ ÎµÎ¹Î´Î¹ÎºÏÎ½ tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(sents, vocab):\n",
    "    result = []\n",
    "    for sent in sents:\n",
    "        replaced = replace_with_unk(sent, vocab)\n",
    "        result.append(['<BOS>'] + replaced + ['<EOS>'])\n",
    "    return result\n",
    "\n",
    "# ğŸ”¹ Î¤ÎµÎ»Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Î­Ï„Î¿Î¹Î¼ÎµÏ‚ Î³Î¹Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± n-grams\n",
    "train_prepared = prepare_sentences(train_sents, vocab)\n",
    "test_prepared = prepare_sentences(test_sents, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î’Î®Î¼Î± 4: Î•Ï€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Î²Î¬ÏƒÎµÎ¹ Î•ÎºÏ†ÏÎ½Î·ÏƒÎ·Ï‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "['<BOS>', '<UNK>', '<UNK>', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', '<EOS>']\n",
      "Pierre ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÎ¹Ï‚: 1\n",
      "Vinken ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÎ¹Ï‚: 2\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])\n",
    "print(train_prepared[0])\n",
    "\n",
    "print(\"Pierre ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÎ¹Ï‚:\", freqs[\"Pierre\"])\n",
    "print(\"Vinken ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÎ¹Ï‚:\", freqs[\"Vinken\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î’Î®Î¼Î± 5: Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î³Î¹Î± Î£Ï…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎ· Î Î¯Î½Î±ÎºÎ± (Î•ÏÏÏ„Î·Î¼Î± 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Original text': {'2-gram (k=1)': 383.74,\n",
       "  '2-gram (k=0.01)': 137.84,\n",
       "  '3-gram (k=1)': 1505.81,\n",
       "  '3-gram (k=0.01)': 464.06},\n",
       " 'Lowercase': {'2-gram (k=1)': 349.46,\n",
       "  '2-gram (k=0.01)': 130.43,\n",
       "  '3-gram (k=1)': 1374.44,\n",
       "  '3-gram (k=0.01)': 427.07},\n",
       " 'Abstract digits': {'2-gram (k=1)': 336.26,\n",
       "  '2-gram (k=0.01)': 122.12,\n",
       "  '3-gram (k=1)': 1319.79,\n",
       "  '3-gram (k=0.01)': 386.26}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import re \n",
    "\n",
    "# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î²Î¿Î·Î¸Î·Ï„Î¹ÎºÎ®Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î® n-grams\n",
    "def extract_ngrams(sentences, n):\n",
    "    \"\"\"\n",
    "    Î”Î­Ï‡ÎµÏ„Î±Î¹ Î»Î¯ÏƒÏ„Î± Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ (ÏŒÏ€Î¿Ï… ÎºÎ¬Î¸Îµ Ï€ÏÏŒÏ„Î±ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î»Î¯ÏƒÏ„Î± Î±Ï€ÏŒ tokens)\n",
    "    ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ ÏŒÎ»Î± Ï„Î± n-grams Ï€Î¿Ï… ÎµÎ¾Î¬Î³Î¿Î½Ï„Î±Î¹ ÏƒÎµ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for sent in sentences:\n",
    "        ngrams.extend([tuple(sent[i:i+n]) for i in range(len(sent)-n+1)])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Lowercase Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î®\n",
    "def to_lowercase(sents):\n",
    "    return [[token.lower() for token in sent] for sent in sents]\n",
    "\n",
    "# Abstract digits Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î®\n",
    "def abstract_digits(sents):\n",
    "    def abstract_token(token):\n",
    "        return re.sub(r'\\d', '#', token)\n",
    "    return [[abstract_token(token) for token in sent] for sent in sents]\n",
    "\n",
    "\n",
    "# Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· n-gram Î¼Î¿Î½Ï„Î­Î»Î¿Ï… Î¼Îµ add-k smoothing\n",
    "class NgramModel:\n",
    "    def __init__(self, n, k, train_data):\n",
    "        \"\"\"\n",
    "        n: Î¼Î­Î³ÎµÎ¸Î¿Ï‚ Ï„Ï‰Î½ n-grams (2 Î³Î¹Î± bigram, 3 Î³Î¹Î± trigram)\n",
    "        k: Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·Ï‚ (add-k)\n",
    "        train_data: Î»Î¯ÏƒÏ„Î± Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ tokens (ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î¼Îµ <UNK>, <BOS>, <EOS>)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Î£Ï…Î»Î»Î¿Î³Î® n-grams ÎºÎ±Î¹ context (Ï€.Ï‡. bigram: (w1, w2), context: w1)\n",
    "        self.ngrams = Counter(extract_ngrams(train_data, n))\n",
    "        self.context = Counter(extract_ngrams(train_data, n - 1))\n",
    "\n",
    "        # Î›ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿ V (Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Î³Î¹Î± smoothing)\n",
    "        self.vocab = set(token for sent in train_data for token in sent)\n",
    "\n",
    "    def prob(self, ngram):\n",
    "        \"\"\"\n",
    "        Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½ Î¼Îµ add-k smoothing\n",
    "        \"\"\"\n",
    "        prefix = ngram[:-1]\n",
    "        return (self.ngrams[ngram] + self.k) / (self.context[prefix] + self.k * len(self.vocab))\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"\n",
    "        Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ perplexity\n",
    "        \"\"\"\n",
    "        ngrams_test = extract_ngrams(test_data, self.n)\n",
    "        N = len(ngrams_test)\n",
    "        log_sum = 0\n",
    "        for ngram in ngrams_test:\n",
    "            p = self.prob(ngram)\n",
    "            log_sum += math.log(p)\n",
    "        return math.exp(-log_sum / N)\n",
    "\n",
    "# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·Ï‚ Î³Î¹Î± ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± (original, lowercase, digits)\n",
    "def evaluate_all_versions(train_raw, test_raw):\n",
    "    versions = {\n",
    "        \"Original text\": (train_raw, test_raw),\n",
    "        \"Lowercase\": (to_lowercase(train_raw), to_lowercase(test_raw)),\n",
    "        \"Abstract digits\": (abstract_digits(train_raw), abstract_digits(test_raw))\n",
    "    }\n",
    "\n",
    "    settings = [(2, 1), (2, 0.01), (3, 1), (3, 0.01)]\n",
    "    results = {v: {} for v in versions}\n",
    "\n",
    "    for version, (train, test) in versions.items():\n",
    "        for n, k in settings:\n",
    "            model = NgramModel(n, k, train)\n",
    "            ppl = model.perplexity(test)\n",
    "            results[version][f'{n}-gram (k={k})'] = round(ppl, 2)\n",
    "\n",
    "    return results\n",
    "\n",
    "evaluate_all_versions(train_prepared, test_prepared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î Î¯Î½Î±ÎºÎ±Ï‚ & Î£Ï…Î¼Ï€ÎµÏÎ¬ÏƒÎ¼Î±Ï„Î± Ï‰Ï‚ Ï€ÏÎ¿Ï‚ Ï„Î¿ Perplexity\n",
    "\n",
    "| Language model        | Original text | Lowercase   | Abstract digits |\n",
    "|----------------------|---------------|-------------|-----------------|\n",
    "| Bigrams (k = 1)      | 383.74        | 349.46      | 336.26          |\n",
    "| Bigrams (k = 0.01)   | 137.84        | 130.43      | 122.12          |\n",
    "| Trigrams (k = 1)     | 1505.81       | 1374.44     | 1319.79         |\n",
    "| Trigrams (k = 0.01)  | 464.06        | 427.07      | 386.26          |\n",
    "  \n",
    "  \n",
    "  \n",
    "- **Î Î¹Î¿ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î±Ï„Î¹ÎºÏŒ Î¼Î¿Î½Ï„Î­Î»Î¿**: Î¤Î± **bigrams** (2-grams) Î¼Îµ **k = 0.01** ÎµÎ¯Ï‡Î±Î½ Ï„Î· **Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· Ï„Î¹Î¼Î® perplexity** ÏƒÎµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Î¼Î¿ÏÏ†Î­Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚.\n",
    "  \n",
    "- **Î¤Î¹Î¼Î® Ï„Î¿Ï… k**: Î— Ï„Î¹Î¼Î® **k = 0.01** ÎµÎ¯Î½Î±Î¹ **ÏƒÎ±Ï†ÏÏ‚ Ï€Î¹Î¿ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î·** Î±Ï€ÏŒ Ï„Î¿ k = 1, Î±Ï†Î¿Ï Î¼ÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î·Î½ ÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ· ÎºÎ±Î¹ Î´Î¯Î½ÎµÎ¹ Ï€Î¹Î¿ ÏÎµÎ±Î»Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚ ÏƒÎµ ÏƒÏ€Î¬Î½Î¹Î± n-grams, Î²ÎµÎ»Ï„Î¹ÏÎ½Î¿Î½Ï„Î±Ï‚ Î­Ï„ÏƒÎ¹ Ï„Î¿ perplexity.\n",
    "\n",
    "- **Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· lowercase**: Î— Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÏ‰Î½ ÏƒÎµ Ï€ÎµÎ¶Î¬ (lowercase) **Î²ÎµÎ»Ï„Î¯Ï‰ÏƒÎµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±** ÏƒÎµ ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î±. Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î±Î½Î±Î¼ÎµÎ½ÏŒÎ¼ÎµÎ½Î¿, ÎºÎ±Î¸ÏÏ‚ ÎµÎ½Î¿Ï€Î¿Î¹ÎµÎ¯ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î¼Î¿ÏÏ†Î­Ï‚ Ï„Î·Ï‚ Î¯Î´Î¹Î±Ï‚ Î»Î­Î¾Î·Ï‚ (Ï€.Ï‡. `The` ÎºÎ±Î¹ `the`), Î¼ÎµÎ¹ÏÎ½Î¿Î½Ï„Î±Ï‚ Ï„Î·Î½ Ï€Î¿Î»Ï…Ï€Î»Î¿ÎºÏŒÏ„Î·Ï„Î± Ï„Î¿Ï… Î»ÎµÎ¾Î¹Î»Î¿Î³Î¯Î¿Ï….\n",
    "\n",
    "- **Î•Ï€Î¯Î´ÏÎ±ÏƒÎ· abstract digits**: Î— Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î±ÏÎ¹Î¸Î¼ÏÎ½ Î¼Îµ Î±Ï†Î·ÏÎ·Î¼Î­Î½Î¿Ï…Ï‚ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚ (`#`) **Î²ÎµÎ»Ï„Î¯Ï‰ÏƒÎµ Ï€ÎµÏÎ±Î¹Ï„Î­ÏÏ‰ Ï„Î¿ perplexity**. ÎŸÎ¹ Î±ÏÎ¹Î¸Î¼Î¿Î¯ ÏƒÏ…Ï‡Î½Î¬ ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ Î¼Î¯Î± Ï†Î¿ÏÎ¬ (hapax legomena), Î¿Ï€ÏŒÏ„Îµ Î· Î±Ï†Î·ÏÎ·Î¼Î­Î½Î· Î±Î½Î±Ï€Î±ÏÎ¬ÏƒÏ„Î±ÏƒÎ· Î¼ÎµÎ¹ÏÎ½ÎµÎ¹ Ï„Î· Î´Î¹Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… Î»ÎµÎ¾Î¹Î»Î¿Î³Î¯Î¿Ï… Ï‡Ï‰ÏÎ¯Ï‚ Î±Ï€ÏÎ»ÎµÎ¹Î± ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ®Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î’Î®Î¼Î± 6: Î Î±ÏÎ±Î³Ï‰Î³Î® Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ (Î•ÏÏÏ„Î·Î¼Î± 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ÎœÎ¿Î½Ï„Î­Î»Î¿ 2-gram Î¼Îµ k=1:\n",
      "--------------------------------------------------\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 1: <BOS> heart stopped medical Carla Carnival Old total start On globe Co plastic foreign number On Says Entertainment Sen. surprisingly Canada compound nearly Mercantile complex directly 51 equipment chemicals <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 2: <BOS> 8.50 N.J eager where remove plunged employees national gives Two suffer gains seeing Several Markey basic roof-crush sense Price disciplinary acne providing century as looming 13 conference few <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 3: <BOS> `` agreement widget people flat section 41 retired 30 So bridge covered multi-crystal turns In wait estimated household won standard expire Reupke becoming Money Co large front Computer <EOS>\n",
      "\n",
      " ÎœÎ¿Î½Ï„Î­Î»Î¿ 2-gram Î¼Îµ k=0.01:\n",
      "--------------------------------------------------\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 1: <BOS> In her , a share . <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 2: <BOS> By earned fueled Trudeau refund card business appears US$ margins approach it to help *-1 always held media withdrawal year ending resolution of two funded premium 9 1\\/2 <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 3: <BOS> And 47 employer looming received letters A. commodity markets Dorrance 2029 200 represent quarterly courts underwriters General 22\\/32 Freeport-McMoRan Lake working Fairless mature fell agency live unlike stand <EOS>\n",
      "\n",
      " ÎœÎ¿Î½Ï„Î­Î»Î¿ 3-gram Î¼Îµ k=1:\n",
      "--------------------------------------------------\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 1: <BOS> 70 court where expressed federal 40 figures * Klauser Law % made Group editorial Morgan simply Ms. court ship financings Although ones himself 12 St. 33 French During <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 2: <BOS> great produce understand Profit Poor pretax as run scheduled ballplayers suggests effective lesson index-arbitrage effects classroom family bulk compliance remained U.S.A. prevent Johnson rules trying men Nasdaq able <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 3: <BOS> lucrative Senate generally markets required respond bankruptcy market respond themselves chairman Guinea damage staff treatment not asks Index elected Moody industry Illuminating Singapore defense promotion *-3 fall impose <EOS>\n",
      "\n",
      " ÎœÎ¿Î½Ï„Î­Î»Î¿ 3-gram Î¼Îµ k=0.01:\n",
      "--------------------------------------------------\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 1: <BOS> accommodate hard Dodge race step parents less 47 compliance disgorge Chevrolet profitable smoking activity smoking designers Many took crisis Terms referred Richard branch larger According techniques Hahn opinion <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 2: <BOS> 30-year clients credit plastic senior district bells risks local three decided early Article *-3 squeeze Smith publicly knew Chicago harder Instead church liquidity disciplinary Acquisition effect big specified <EOS>\n",
      "* Î ÏÏŒÏ„Î±ÏƒÎ· 3: <BOS> nine dismissed statute rating heads debts total collected commodity required one-third beaten political widespread large Donald 1 noted hours 2009 hundred entered strategy quick N.C York-based charge France <EOS>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(model, max_length=30, verbose=False):\n",
    "    \"\"\"\n",
    "    Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï€ÏÏŒÏ„Î±ÏƒÎ· Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ Î´Î¿Î¸Î­Î½ Î¼Î¿Î½Ï„Î­Î»Î¿ n-gram.\n",
    "    Î•Ï€Î¹Î»Î­Î³ÎµÎ¹ Ï„Ï…Ï‡Î±Î¯Î± ÎµÏ€ÏŒÎ¼ÎµÎ½Î· Î»Î­Î¾Î· Î±Î½Î¬Î»Î¿Î³Î± Î¼Îµ Ï„Î·Î½ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î±.\n",
    "    - model: Î±Î½Ï„Î¹ÎºÎµÎ¯Î¼ÎµÎ½Î¿ NgramModel\n",
    "    - max_length: Î¼Î­Î³Î¹ÏƒÏ„Î¿ Î¼Î®ÎºÎ¿Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚\n",
    "    - verbose: Î±Î½ ÎµÎ¯Î½Î±Î¹ True, ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÎ¹ Ï„Î¹Ï‚ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚ ÎµÏ€Î¹Î»Î¿Î³Î®Ï‚\n",
    "    \"\"\"\n",
    "    sentence = ['<BOS>']\n",
    "\n",
    "    while len(sentence) < max_length:\n",
    "        context = tuple(sentence[-(model.n - 1):]) if model.n > 1 else tuple()\n",
    "        \n",
    "        # Î¥Ï€Î¿ÏˆÎ®Ï†Î¹Î± ÎµÏ€ÏŒÎ¼ÎµÎ½Î± tokens Î±Ï€ÏŒ Î»ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿ (ÎµÎºÏ„ÏŒÏ‚ <UNK>)\n",
    "        candidates = [w for w in model.vocab if w != '<UNK>']\n",
    "        \n",
    "        # Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½ Î³Î¹Î± ÎºÎ¬Î¸Îµ Ï…Ï€Î¿ÏˆÎ®Ï†Î¹Î¿\n",
    "        probs = []\n",
    "        for word in candidates:\n",
    "            ngram = context + (word,)\n",
    "            prob = model.prob(ngram)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        # ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï€Î¹Î¸Î±Î½Î¿Ï„Î®Ï„Ï‰Î½\n",
    "        total = sum(probs)\n",
    "        probs = [p / total for p in probs]\n",
    "\n",
    "        # Î¤Ï…Ï‡Î±Î¯Î± ÎµÏ€Î¹Î»Î¿Î³Î® Î»Î­Î¾Î·Ï‚ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¹Ï‚ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„ÎµÏ‚\n",
    "        next_word = random.choices(candidates, weights=probs, k=1)[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Context: {context} -> Î•Ï€Î¹Î»Î¿Î³Î®: '{next_word}' (Prob={round(max(probs), 4)})\")\n",
    "\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        if next_word == '<EOS>':\n",
    "            break\n",
    "\n",
    "    return ' '.join(sentence[1:-1])  # Î±Ï†Î±Î¯ÏÎµÏƒÎ· <BOS>, <EOS> Î±Ï€ÏŒ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·\n",
    "\n",
    "# ğŸ”¹ Î£Ï…Î½Î´Ï…Î±ÏƒÎ¼Î¿Î¯ Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹\n",
    "combinations = [\n",
    "    (2, 1),     # Bigram, k=1\n",
    "    (2, 0.01),  # Bigram, k=0.01\n",
    "    (3, 1),     # Trigram, k=1\n",
    "    (3, 0.01)   # Trigram, k=0.01\n",
    "]\n",
    "\n",
    "# ğŸ”¹ Î Î±ÏÎ±Î³Ï‰Î³Î® Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½\n",
    "for n, k in combinations:\n",
    "    print(f\"\\n ÎœÎ¿Î½Ï„Î­Î»Î¿ {n}-gram Î¼Îµ k={k}:\\n\" + \"-\"*50)\n",
    "    model = NgramModel(n, k, train_prepared)\n",
    "    for i in range(1, 4):  # 3 Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ Î¼Î¿Î½Ï„Î­Î»Î¿\n",
    "        sentence = generate_sentence(model)\n",
    "        print(f\"* Î ÏÏŒÏ„Î±ÏƒÎ· {i}: <BOS> {''.join(sentence)} <EOS>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Î Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÎ¹Ï‚ & Î¥Ï€Î¿Î¸Î­ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Ï„Î·Î½ Î Î¿Î¹ÏŒÏ„Î·Ï„Î± Ï„Ï‰Î½ Î Î±ÏÎ±Î³ÏŒÎ¼ÎµÎ½Ï‰Î½ Î ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½\n",
    "\n",
    "ÎšÎ±Ï„Î¬ Ï„Î· Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ Î¼Îµ Ï‡ÏÎ®ÏƒÎ· Ï„Ï‰Î½ n-gram Î¼Î¿Î½Ï„Î­Î»Ï‰Î½ (Bigram/Trigram Î¼Îµ Ï„Î¹Î¼Î­Ï‚ k=1 ÎºÎ±Î¹ k=0.01), Ï€Î±ÏÎ±Ï„Î·ÏÎ®Î¸Î·ÎºÎµ ÏŒÏ„Î¹:\n",
    "\n",
    "- ÎŸÎ¹ Ï€Î±ÏÎ±Î³ÏŒÎ¼ÎµÎ½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ **Î´ÎµÎ½ Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶Î¿Ï…Î½ ÏƒÏ…Î½Ï„Î±ÎºÏ„Î¹ÎºÎ® ÏƒÏ…Î½Î¿Ï‡Î®** ÎºÎ±Î¹ **ÏƒÏ…Ï‡Î½Î¬ Î´ÎµÎ½ Î²Î³Î¬Î¶Î¿Ï…Î½ Î½ÏŒÎ·Î¼Î±** ÏƒÏ„Î· Ï†Ï…ÏƒÎ¹ÎºÎ® Î³Î»ÏÏƒÏƒÎ±.\n",
    "- Î Î¿Î»Î»Î­Ï‚ Î±Ï€ÏŒ Î±Ï…Ï„Î­Ï‚ Î¼Î¿Î¹Î¬Î¶Î¿Ï…Î½ Î¼Îµ **Ï„Ï…Ï‡Î±Î¯Î± Î±ÎºÎ¿Î»Î¿Ï…Î¸Î¯Î± Î»Î­Î¾ÎµÏ‰Î½**, Ï‡Ï‰ÏÎ¯Ï‚ Î»Î¿Î³Î¹ÎºÎ® ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î± Î® Î½Î¿Î·Î¼Î±Ï„Î¹ÎºÎ® ÏƒÏÎ½Î´ÎµÏƒÎ·.\n",
    "- Î£Îµ Î±ÏÎºÎµÏ„Î­Ï‚ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚ ÎµÎ¼Ï†Î±Î½Î¯Î¶Î¿Î½Ï„Î±Î¹ **Î±ÏƒÏ…Î½Î®Î¸Î¹ÏƒÏ„Î¿Î¹ ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼Î¿Î¯**, Ï€.Ï‡. \"`smoking activity smoking designers`\", Î® Î»Î­Î¾ÎµÎ¹Ï‚ Ï€Î¿Ï… Î´ÎµÎ½ Î¸Î± ÏƒÏ…Î½Î±Î½Ï„Î¿ÏÏƒÎ±Î¼Îµ ÎµÏÎºÎ¿Î»Î± Î´Î¹Î±Î´Î¿Ï‡Î¹ÎºÎ¬ ÏƒÎµ Ï†Ï…ÏƒÎ¹ÎºÎ® Ï‡ÏÎ®ÏƒÎ·.\n",
    "\n",
    "#### ğŸ“Œ Î Î¹Î¸Î±Î½Î­Ï‚ Î±Î¹Ï„Î¯ÎµÏ‚ Î³Î¹Î± Ï„Î± Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ Ï†Î±Î¹Î½ÏŒÎ¼ÎµÎ½Î±:\n",
    "\n",
    "- ÎŠÏƒÏ‰Ï‚ ÎµÏ€ÎµÎ¹Î´Î® Ï„Î± n-gram Î¼Î¿Î½Ï„Î­Î»Î± **Î»Î±Î¼Î²Î¬Î½Î¿Ï…Î½ Ï…Ï€ÏŒÏˆÎ· Î¼ÏŒÎ½Î¿ Ï„Î± Ï„Î¿Ï€Î¹ÎºÎ¬ ÏƒÏ…Î¼Ï†ÏÎ±Î¶ÏŒÎ¼ÎµÎ½Î±** (Ï€.Ï‡. Î¼Î¯Î± Î® Î´ÏÎ¿ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ Î»Î­Î¾ÎµÎ¹Ï‚) ÎºÎ±Î¹ ÏŒÏ‡Î¹ Ï„Î¿ ÎµÏ…ÏÏÏ„ÎµÏÎ¿ Î½ÏŒÎ·Î¼Î± Ï„Î·Ï‚ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.\n",
    "- Î•Ï€Î¯ÏƒÎ·Ï‚, Ï€Î¹Î¸Î±Î½ÏÏ‚ Î· **Ï„Ï…Ï‡Î±Î¯Î± ÎµÏ€Î¹Î»Î¿Î³Î® Ï„Î·Ï‚ Î±ÏÏ‡Î¹ÎºÎ®Ï‚ Î»Î­Î¾Î·Ï‚ Î¼ÎµÏ„Î¬ Ï„Î¿ `<BOS>`** Î½Î± ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Ï„Î·Î½ ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· Î¼Îµ Î»Î­Î¾ÎµÎ¹Ï‚ Ï€Î¿Ï… Î´Îµ ÏƒÏ…Î½Î¬Î´Î¿Ï…Î½ Î¼Îµ Ï†Ï…ÏƒÎ¹ÎºÎ® ÏÎ¿Î® Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.\n",
    "- Î— Ï„Î¹Î¼Î® **k=1** ÎµÎ½Î´ÎµÏ‡Î¿Î¼Î­Î½Ï‰Ï‚ Î¿Î´Î·Î³ÎµÎ¯ ÏƒÎµ **Ï…Ï€ÎµÏÎµÎ¾Î¿Î¼Î¬Î»Ï…Î½ÏƒÎ·**, Î´Î·Î»Î±Î´Î® Î±Ï€Î¿Î´Ï…Î½Î¬Î¼Ï‰ÏƒÎ· Ï„Î·Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±Ï‚ Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Ï„Î¿ Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÏŒ corpus, ÎµÎ½Î¹ÏƒÏ‡ÏÎ¿Î½Ï„Î±Ï‚ ÏƒÏ€Î¬Î½Î¹Î± n-grams.\n",
    "- Î‘Î½ ÎºÎ±Î¹ Ï„Î± **trigram Î¼Î¿Î½Ï„Î­Î»Î±** Î²Î±ÏƒÎ¯Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ± ÏƒÏ…Î¼Ï†ÏÎ±Î¶ÏŒÎ¼ÎµÎ½Î±, Î±Ï…Ï„ÏŒ **Î´ÎµÎ½ Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Î½Î± ÎµÏ€Î±ÏÎºÎµÎ¯** ÏÏƒÏ„Îµ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸Î¿ÏÎ½ Ï†Ï…ÏƒÎ¹ÎºÎ­Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚, ÎµÎ½Î´ÎµÏ‡Î¿Î¼Î­Î½Ï‰Ï‚ Î»ÏŒÎ³Ï‰ Ï€ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î¿Ï… Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ Î® Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î±Ï‚ Ï„Î¿Ï… training corpus.\n",
    "\n",
    "Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬, Î¿Î¹ Ï€Î±ÏÎ±Î³ÏŒÎ¼ÎµÎ½ÎµÏ‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ Ï€Î±ÏÎ­Ï‡Î¿Ï…Î½ ÎµÎ½Î´ÎµÎ¯Î¾ÎµÎ¹Ï‚ ÏŒÏ„Î¹ Ï„Î± ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ n-gram Î¼Î¿Î½Ï„Î­Î»Î±, Ï‡Ï‰ÏÎ¯Ï‚ Î²Î±Î¸ÏÏ„ÎµÏÎ· ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·, **Î´Ï…ÏƒÎºÎ¿Î»ÎµÏÎ¿Î½Ï„Î±Î¹ Î½Î± Ï€Î±ÏÎ±Î³Î¬Î³Î¿Ï…Î½ Ï†Ï…ÏƒÎ¹ÎºÏŒ ÎºÎ±Î¹ ÏƒÏ…Î½ÎµÎºÏ„Î¹ÎºÏŒ Î»ÏŒÎ³Î¿**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
