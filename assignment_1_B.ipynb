{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ΕΠΕΞΕΡΓΑΣΙΑ ΦΥΣΙΚΗΣ ΓΛΩΣΣΑΣ - Εργασία 1\n",
    "## Β. N-gram Language Models\n",
    "**Μάθημα:**  Επεξεργασία Φυσικής Γλώσσας \n",
    "\n",
    "**Συγγραφέας:** Ιωάννης Kουτσούκης \n",
    "\n",
    "**Εκδόσεις:** 2025-03-22 | v.0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📁 Βήμα 1: Φόρτωση και Διαχωρισμός του Corpus\n",
    "\n",
    "Χρησιμοποιούμε το corpus `treebank` από το NLTK, το οποίο περιέχει 199 αρχεία ειδήσεων από την Wall Street Journal. Διαχωρίζουμε τα δεδομένα σε:\n",
    "\n",
    "- **170 αρχεία** για εκπαίδευση (training set).\n",
    "- **29 αρχεία** για αξιολόγηση (test set).\n",
    "\n",
    "Το corpus είναι ήδη χωρισμένο σε tokens και προτάσεις.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/ioanniskoutsoukis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Πλήθος προτάσεων εκπαίδευσης: 3509\n",
      "Πλήθος προτάσεων αξιολόγησης: 405\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter\n",
    "\n",
    "# 📌 Κατέβασμα corpus treebank αν δεν υπάρχει\n",
    "nltk.download('treebank')\n",
    "\n",
    "# 🔹 Λίστα με τα 199 αρχεία\n",
    "files = treebank.fileids()\n",
    "\n",
    "# 🔹 Διαχωρισμός σε train και test\n",
    "train_files = files[:170]\n",
    "test_files = files[170:]\n",
    "\n",
    "# 🔹 Εξαγωγή προτάσεων\n",
    "train_sents = [sent for file in train_files for sent in treebank.sents(file)]\n",
    "test_sents = [sent for file in test_files for sent in treebank.sents(file)]\n",
    "\n",
    "print(f\"Πλήθος προτάσεων εκπαίδευσης: {len(train_sents)}\")\n",
    "print(f\"Πλήθος προτάσεων αξιολόγησης: {len(test_sents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2: Δημιουργία Λεξιλογίου και Αντικατάσταση με `<UNK>`\n",
    "\n",
    "Υπολογίζουμε τις συχνότητες εμφάνισης όλων των tokens από το σύνολο εκπαίδευσης και κατασκευάζουμε το λεξιλόγιο **L**:\n",
    "\n",
    "- Τα tokens που εμφανίζονται **λιγότερες από 3 φορές** αντικαθίστανται με `<UNK>`.\n",
    "- Τα υπόλοιπα περιλαμβάνονται στο τελικό λεξιλόγιο **L**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Συγκέντρωση όλων των tokens από τα κείμενα εκπαίδευσης\n",
    "train_tokens = [token for sent in train_sents for token in sent]\n",
    "\n",
    "# 🔹 Μέτρηση συχνοτήτων\n",
    "freqs = Counter(train_tokens)\n",
    "\n",
    "# 🔹 Κατασκευή λεξιλογίου: κρατάμε ό,τι έχει συχνότητα ≥ 3\n",
    "vocab = {word for word, count in freqs.items() if count >= 3}\n",
    "\n",
    "# 🔹 Συνάρτηση αντικατάστασης <UNK>\n",
    "def replace_with_unk(sent, vocab):\n",
    "    return [token if token in vocab else \"<UNK>\" for token in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3: Προετοιμασία Προτάσεων με `<BOS>` και `<EOS>`\n",
    "\n",
    "Κάθε πρόταση περιβάλλεται από τα ειδικά tokens:\n",
    "\n",
    "- `<BOS>` στην αρχή της πρότασης.\n",
    "- `<EOS>` στο τέλος της πρότασης.\n",
    "\n",
    "Αυτό μας επιτρέπει να δημιουργήσουμε ν-γράμματα που σέβονται τα όρια της πρότασης. Η αντικατάσταση με `<UNK>` γίνεται πριν την προσθήκη των ειδικών tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentences(sents, vocab):\n",
    "    result = []\n",
    "    for sent in sents:\n",
    "        replaced = replace_with_unk(sent, vocab)\n",
    "        result.append(['<BOS>'] + replaced + ['<EOS>'])\n",
    "    return result\n",
    "\n",
    "# 🔹 Τελικές προτάσεις έτοιμες για δημιουργία n-grams\n",
    "train_prepared = prepare_sentences(train_sents, vocab)\n",
    "test_prepared = prepare_sentences(test_sents, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4: Επιβεβαίωση αποτελεσμάτων βάσει Εκφώνησης\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "['<BOS>', '<UNK>', '<UNK>', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', '<EOS>']\n",
      "Pierre εμφανίσεις: 1\n",
      "Vinken εμφανίσεις: 2\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])\n",
    "print(train_prepared[0])\n",
    "\n",
    "print(\"Pierre εμφανίσεις:\", freqs[\"Pierre\"])\n",
    "print(\"Vinken εμφανίσεις:\", freqs[\"Vinken\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5: Προετοιμασία για Συμπλήρωση Πίνακα (Ερώτημα 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Original text': {'2-gram (k=1)': 383.74,\n",
       "  '2-gram (k=0.01)': 137.84,\n",
       "  '3-gram (k=1)': 1505.81,\n",
       "  '3-gram (k=0.01)': 464.06},\n",
       " 'Lowercase': {'2-gram (k=1)': 349.46,\n",
       "  '2-gram (k=0.01)': 130.43,\n",
       "  '3-gram (k=1)': 1374.44,\n",
       "  '3-gram (k=0.01)': 427.07},\n",
       " 'Abstract digits': {'2-gram (k=1)': 336.26,\n",
       "  '2-gram (k=0.01)': 122.12,\n",
       "  '3-gram (k=1)': 1319.79,\n",
       "  '3-gram (k=0.01)': 386.26}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import re \n",
    "\n",
    "# Ορισμός βοηθητικής συνάρτησης για την εξαγωγή n-grams\n",
    "def extract_ngrams(sentences, n):\n",
    "    \"\"\"\n",
    "    Δέχεται λίστα προτάσεων (όπου κάθε πρόταση είναι λίστα από tokens)\n",
    "    και επιστρέφει όλα τα n-grams που εξάγονται σε επίπεδο πρότασης.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for sent in sentences:\n",
    "        ngrams.extend([tuple(sent[i:i+n]) for i in range(len(sent)-n+1)])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "# Lowercase μετατροπή\n",
    "def to_lowercase(sents):\n",
    "    return [[token.lower() for token in sent] for sent in sents]\n",
    "\n",
    "# Abstract digits μετατροπή\n",
    "def abstract_digits(sents):\n",
    "    def abstract_token(token):\n",
    "        return re.sub(r'\\d', '#', token)\n",
    "    return [[abstract_token(token) for token in sent] for sent in sents]\n",
    "\n",
    "\n",
    "# Εκπαίδευση n-gram μοντέλου με add-k smoothing\n",
    "class NgramModel:\n",
    "    def __init__(self, n, k, train_data):\n",
    "        \"\"\"\n",
    "        n: μέγεθος των n-grams (2 για bigram, 3 για trigram)\n",
    "        k: παράμετρος εξομάλυνσης (add-k)\n",
    "        train_data: λίστα προτάσεων με tokens (επεξεργασμένα με <UNK>, <BOS>, <EOS>)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Συλλογή n-grams και context (π.χ. bigram: (w1, w2), context: w1)\n",
    "        self.ngrams = Counter(extract_ngrams(train_data, n))\n",
    "        self.context = Counter(extract_ngrams(train_data, n - 1))\n",
    "\n",
    "        # Λεξιλόγιο V (χρησιμοποιείται για smoothing)\n",
    "        self.vocab = set(token for sent in train_data for token in sent)\n",
    "\n",
    "    def prob(self, ngram):\n",
    "        \"\"\"\n",
    "        Υπολογισμός πιθανοτήτων με add-k smoothing\n",
    "        \"\"\"\n",
    "        prefix = ngram[:-1]\n",
    "        return (self.ngrams[ngram] + self.k) / (self.context[prefix] + self.k * len(self.vocab))\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"\n",
    "        Υπολογισμός perplexity\n",
    "        \"\"\"\n",
    "        ngrams_test = extract_ngrams(test_data, self.n)\n",
    "        N = len(ngrams_test)\n",
    "        log_sum = 0\n",
    "        for ngram in ngrams_test:\n",
    "            p = self.prob(ngram)\n",
    "            log_sum += math.log(p)\n",
    "        return math.exp(-log_sum / N)\n",
    "\n",
    "# Συνάρτηση αξιολόγησης για όλα τα μοντέλα (original, lowercase, digits)\n",
    "def evaluate_all_versions(train_raw, test_raw):\n",
    "    versions = {\n",
    "        \"Original text\": (train_raw, test_raw),\n",
    "        \"Lowercase\": (to_lowercase(train_raw), to_lowercase(test_raw)),\n",
    "        \"Abstract digits\": (abstract_digits(train_raw), abstract_digits(test_raw))\n",
    "    }\n",
    "\n",
    "    settings = [(2, 1), (2, 0.01), (3, 1), (3, 0.01)]\n",
    "    results = {v: {} for v in versions}\n",
    "\n",
    "    for version, (train, test) in versions.items():\n",
    "        for n, k in settings:\n",
    "            model = NgramModel(n, k, train)\n",
    "            ppl = model.perplexity(test)\n",
    "            results[version][f'{n}-gram (k={k})'] = round(ppl, 2)\n",
    "\n",
    "    return results\n",
    "\n",
    "evaluate_all_versions(train_prepared, test_prepared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Πίνακας & Συμπεράσματα ως προς το Perplexity\n",
    "\n",
    "| Language model        | Original text | Lowercase   | Abstract digits |\n",
    "|----------------------|---------------|-------------|-----------------|\n",
    "| Bigrams (k = 1)      | 383.74        | 349.46      | 336.26          |\n",
    "| Bigrams (k = 0.01)   | 137.84        | 130.43      | 122.12          |\n",
    "| Trigrams (k = 1)     | 1505.81       | 1374.44     | 1319.79         |\n",
    "| Trigrams (k = 0.01)  | 464.06        | 427.07      | 386.26          |\n",
    "  \n",
    "  \n",
    "  \n",
    "- **Πιο αποτελεσματικό μοντέλο**: Τα **bigrams** (2-grams) με **k = 0.01** είχαν τη **χαμηλότερη τιμή perplexity** σε όλες τις μορφές επεξεργασίας.\n",
    "  \n",
    "- **Τιμή του k**: Η τιμή **k = 0.01** είναι **σαφώς πιο κατάλληλη** από το k = 1, αφού μειώνει την εξομάλυνση και δίνει πιο ρεαλιστικές πιθανότητες σε σπάνια n-grams, βελτιώνοντας έτσι το perplexity.\n",
    "\n",
    "- **Επίδραση lowercase**: Η μετατροπή όλων των χαρακτήρων σε πεζά (lowercase) **βελτίωσε τα αποτελέσματα** σε όλα τα μοντέλα. Αυτό είναι αναμενόμενο, καθώς ενοποιεί διαφορετικές μορφές της ίδιας λέξης (π.χ. `The` και `the`), μειώνοντας την πολυπλοκότητα του λεξιλογίου.\n",
    "\n",
    "- **Επίδραση abstract digits**: Η αντικατάσταση αριθμών με αφηρημένους χαρακτήρες (`#`) **βελτίωσε περαιτέρω το perplexity**. Οι αριθμοί συχνά εμφανίζονται μία φορά (hapax legomena), οπότε η αφηρημένη αναπαράσταση μειώνει τη διάσταση του λεξιλογίου χωρίς απώλεια σημασιολογικής πληροφορίας.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 6: Παραγωγή Προτάσεων (Ερώτημα 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Μοντέλο 2-gram με k=1:\n",
      "--------------------------------------------------\n",
      "* Πρόταση 1: <BOS> heart stopped medical Carla Carnival Old total start On globe Co plastic foreign number On Says Entertainment Sen. surprisingly Canada compound nearly Mercantile complex directly 51 equipment chemicals <EOS>\n",
      "* Πρόταση 2: <BOS> 8.50 N.J eager where remove plunged employees national gives Two suffer gains seeing Several Markey basic roof-crush sense Price disciplinary acne providing century as looming 13 conference few <EOS>\n",
      "* Πρόταση 3: <BOS> `` agreement widget people flat section 41 retired 30 So bridge covered multi-crystal turns In wait estimated household won standard expire Reupke becoming Money Co large front Computer <EOS>\n",
      "\n",
      " Μοντέλο 2-gram με k=0.01:\n",
      "--------------------------------------------------\n",
      "* Πρόταση 1: <BOS> In her , a share . <EOS>\n",
      "* Πρόταση 2: <BOS> By earned fueled Trudeau refund card business appears US$ margins approach it to help *-1 always held media withdrawal year ending resolution of two funded premium 9 1\\/2 <EOS>\n",
      "* Πρόταση 3: <BOS> And 47 employer looming received letters A. commodity markets Dorrance 2029 200 represent quarterly courts underwriters General 22\\/32 Freeport-McMoRan Lake working Fairless mature fell agency live unlike stand <EOS>\n",
      "\n",
      " Μοντέλο 3-gram με k=1:\n",
      "--------------------------------------------------\n",
      "* Πρόταση 1: <BOS> 70 court where expressed federal 40 figures * Klauser Law % made Group editorial Morgan simply Ms. court ship financings Although ones himself 12 St. 33 French During <EOS>\n",
      "* Πρόταση 2: <BOS> great produce understand Profit Poor pretax as run scheduled ballplayers suggests effective lesson index-arbitrage effects classroom family bulk compliance remained U.S.A. prevent Johnson rules trying men Nasdaq able <EOS>\n",
      "* Πρόταση 3: <BOS> lucrative Senate generally markets required respond bankruptcy market respond themselves chairman Guinea damage staff treatment not asks Index elected Moody industry Illuminating Singapore defense promotion *-3 fall impose <EOS>\n",
      "\n",
      " Μοντέλο 3-gram με k=0.01:\n",
      "--------------------------------------------------\n",
      "* Πρόταση 1: <BOS> accommodate hard Dodge race step parents less 47 compliance disgorge Chevrolet profitable smoking activity smoking designers Many took crisis Terms referred Richard branch larger According techniques Hahn opinion <EOS>\n",
      "* Πρόταση 2: <BOS> 30-year clients credit plastic senior district bells risks local three decided early Article *-3 squeeze Smith publicly knew Chicago harder Instead church liquidity disciplinary Acquisition effect big specified <EOS>\n",
      "* Πρόταση 3: <BOS> nine dismissed statute rating heads debts total collected commodity required one-third beaten political widespread large Donald 1 noted hours 2009 hundred entered strategy quick N.C York-based charge France <EOS>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(model, max_length=30, verbose=False):\n",
    "    \"\"\"\n",
    "    Δημιουργεί πρόταση με βάση το δοθέν μοντέλο n-gram.\n",
    "    Επιλέγει τυχαία επόμενη λέξη ανάλογα με την πιθανότητα.\n",
    "    - model: αντικείμενο NgramModel\n",
    "    - max_length: μέγιστο μήκος πρότασης\n",
    "    - verbose: αν είναι True, εμφανίζει τις πιθανότητες επιλογής\n",
    "    \"\"\"\n",
    "    sentence = ['<BOS>']\n",
    "\n",
    "    while len(sentence) < max_length:\n",
    "        context = tuple(sentence[-(model.n - 1):]) if model.n > 1 else tuple()\n",
    "        \n",
    "        # Υποψήφια επόμενα tokens από λεξιλόγιο (εκτός <UNK>)\n",
    "        candidates = [w for w in model.vocab if w != '<UNK>']\n",
    "        \n",
    "        # Υπολογισμός πιθανοτήτων για κάθε υποψήφιο\n",
    "        probs = []\n",
    "        for word in candidates:\n",
    "            ngram = context + (word,)\n",
    "            prob = model.prob(ngram)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        # Κανονικοποίηση πιθανοτήτων\n",
    "        total = sum(probs)\n",
    "        probs = [p / total for p in probs]\n",
    "\n",
    "        # Τυχαία επιλογή λέξης με βάση τις πιθανότητες\n",
    "        next_word = random.choices(candidates, weights=probs, k=1)[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Context: {context} -> Επιλογή: '{next_word}' (Prob={round(max(probs), 4)})\")\n",
    "\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        if next_word == '<EOS>':\n",
    "            break\n",
    "\n",
    "    return ' '.join(sentence[1:-1])  # αφαίρεση <BOS>, <EOS> από εμφάνιση\n",
    "\n",
    "# 🔹 Συνδυασμοί που απαιτούνται\n",
    "combinations = [\n",
    "    (2, 1),     # Bigram, k=1\n",
    "    (2, 0.01),  # Bigram, k=0.01\n",
    "    (3, 1),     # Trigram, k=1\n",
    "    (3, 0.01)   # Trigram, k=0.01\n",
    "]\n",
    "\n",
    "# 🔹 Παραγωγή προτάσεων\n",
    "for n, k in combinations:\n",
    "    print(f\"\\n Μοντέλο {n}-gram με k={k}:\\n\" + \"-\"*50)\n",
    "    model = NgramModel(n, k, train_prepared)\n",
    "    for i in range(1, 4):  # 3 προτάσεις για κάθε μοντέλο\n",
    "        sentence = generate_sentence(model)\n",
    "        print(f\"* Πρόταση {i}: <BOS> {''.join(sentence)} <EOS>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Παρατηρήσεις & Υποθέσεις για την Ποιότητα των Παραγόμενων Προτάσεων\n",
    "\n",
    "Κατά τη δημιουργία προτάσεων με χρήση των n-gram μοντέλων (Bigram/Trigram με τιμές k=1 και k=0.01), παρατηρήθηκε ότι:\n",
    "\n",
    "- Οι παραγόμενες προτάσεις **δεν παρουσιάζουν συντακτική συνοχή** και **συχνά δεν βγάζουν νόημα** στη φυσική γλώσσα.\n",
    "- Πολλές από αυτές μοιάζουν με **τυχαία ακολουθία λέξεων**, χωρίς λογική συνέχεια ή νοηματική σύνδεση.\n",
    "- Σε αρκετές περιπτώσεις εμφανίζονται **ασυνήθιστοι συνδυασμοί**, π.χ. \"`smoking activity smoking designers`\", ή λέξεις που δεν θα συναντούσαμε εύκολα διαδοχικά σε φυσική χρήση.\n",
    "\n",
    "#### 📌 Πιθανές αιτίες για τα παραπάνω φαινόμενα:\n",
    "\n",
    "- Ίσως επειδή τα n-gram μοντέλα **λαμβάνουν υπόψη μόνο τα τοπικά συμφραζόμενα** (π.χ. μία ή δύο προηγούμενες λέξεις) και όχι το ευρύτερο νόημα της πρότασης.\n",
    "- Επίσης, πιθανώς η **τυχαία επιλογή της αρχικής λέξης μετά το `<BOS>`** να επιτρέπει την εκκίνηση με λέξεις που δε συνάδουν με φυσική ροή πρότασης.\n",
    "- Η τιμή **k=1** ενδεχομένως οδηγεί σε **υπερεξομάλυνση**, δηλαδή αποδυνάμωση της πληροφορίας που έχει το πραγματικό corpus, ενισχύοντας σπάνια n-grams.\n",
    "- Αν και τα **trigram μοντέλα** βασίζονται σε μεγαλύτερα συμφραζόμενα, αυτό **δεν φαίνεται να επαρκεί** ώστε να δημιουργηθούν φυσικές προτάσεις, ενδεχομένως λόγω περιορισμένου μεγέθους ή ποικιλίας του training corpus.\n",
    "\n",
    "Συνολικά, οι παραγόμενες προτάσεις παρέχουν ενδείξεις ότι τα στατιστικά n-gram μοντέλα, χωρίς βαθύτερη σημασιολογική κατανόηση, **δυσκολεύονται να παραγάγουν φυσικό και συνεκτικό λόγο**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
